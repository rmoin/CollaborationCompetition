{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, we will make use of the Unity ML-Agents environment.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages. Please make sure to install [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Don't need to run this cell. Check out the dependencies in README file\n",
    "#!pip -q install ./python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Don't need to run this cell. This is only required during the training in Udacity workspaces\n",
    "#from workspace_utils import active_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils as U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Actor, Critic\n",
    "from utils import sample, update_targets, OUNoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameters\n",
    "\n",
    "## For Experience Replay\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 128         # minibatch size\n",
    "\n",
    "## For Fixed-Q Target\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "UPDATE_EVERY = 1       # how often to update the network. \n",
    "UPDATE_TIMES = 1       # and how many times to update\n",
    "\n",
    "# For Cumulative Reward\n",
    "GAMMA = 0.99            # discount factor\n",
    "\n",
    "## For Q Network\n",
    "LR_ACTOR = 1e-3         # learning rate of the actor\n",
    "LR_CRITIC = 1e-3        # learning rate of the critic\n",
    "\n",
    "WEIGHT_DECAY = 0        # L2 weight decay\n",
    "\n",
    "OU_SIGMA = 0.2          # Ornstein-Uhlenbeck noise parameter\n",
    "OU_THETA = 0.15         # Ornstein-Uhlenbeck noise parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = 24\n",
    "action_size = 2\n",
    "random_seed = 1\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## AGENT No. 1 \n",
    "\n",
    "# Actor Network (w/ Target Network)\n",
    "actor_local1 = Actor(state_size+1, action_size, random_seed).to(device)\n",
    "actor_target1 = Actor(state_size+1, action_size, random_seed).to(device)\n",
    "actor_optimizer1 = optim.Adam(actor_local1.parameters(), lr=LR_ACTOR)\n",
    "\n",
    "# Critic Network (w/ Target Network)\n",
    "critic_local1 = Critic(state_size+1, action_size, random_seed).to(device)\n",
    "critic_target1 = Critic(state_size+1, action_size, random_seed).to(device)\n",
    "critic_optimizer1 = optim.Adam(critic_local1.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## AGENT No. 2 \n",
    "\n",
    "# Actor Network (w/ Target Network)\n",
    "actor_local2 = Actor(state_size+1, action_size, random_seed).to(device)\n",
    "actor_target2 = Actor(state_size+1, action_size, random_seed).to(device)\n",
    "actor_optimizer2 = optim.Adam(actor_local2.parameters(), lr=LR_ACTOR)\n",
    "\n",
    "\n",
    "# Critic Network (w/ Target Network)\n",
    "critic_local2 = Critic(state_size+1, action_size, random_seed).to(device)\n",
    "critic_target2 = Critic(state_size+1, action_size, random_seed).to(device)\n",
    "critic_optimizer2 = optim.Adam(critic_local2.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_targets(critic_local1, critic_target1)\n",
    "update_targets(actor_local1, actor_target1)        \n",
    "update_targets(critic_local2, critic_target2)\n",
    "update_targets(actor_local2, actor_target2)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some util and data variables\n",
    "mu = 0.\n",
    "replay_memory = deque(maxlen=BUFFER_SIZE) # same memory will be used for both the agents to implement MADDPG\n",
    "experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "# Noise process\n",
    "noise = OUNoise(action_size, random_seed, mu, OU_SIGMA, OU_THETA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Run this cell in Udacity workspace. Otherwise run the cell below\n",
    "\n",
    "#env = UnityEnvironment(file_name=\"/data/Tennis_Linux_NoVis/Tennis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "#Run this cell locally on mac only. Otherwise run the above cell.\n",
    "\n",
    "env = UnityEnvironment(file_name=\"Tennis.app\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment (Optional Step)\n",
    "\n",
    "In the next code cell, we will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, we will watch the agent's performance, if it selects an action at random with each time step.  A window should pop up that allows us to observe the agent, as it moves through the environment.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: -0.004999999888241291\n",
      "Total steps in this episode: 14\n",
      "Total score (averaged over agents) this episode: -0.004999999888241291\n",
      "Total steps in this episode: 17\n",
      "Total score (averaged over agents) this episode: -0.004999999888241291\n",
      "Total steps in this episode: 15\n",
      "Total score (averaged over agents) this episode: -0.004999999888241291\n",
      "Total steps in this episode: 15\n",
      "Total score (averaged over agents) this episode: -0.004999999888241291\n",
      "Total steps in this episode: 14\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):                                         # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    steps = 0\n",
    "    while True:\n",
    "        actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "        actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        steps += 1\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))\n",
    "    print('Total steps in this episode: {}'.format(steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Define Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Agent - one of the core logic of DDPG - To run INFERENCE on Actor Neural Network\n",
    "def agent(actor_local, state, add_noise=True):\n",
    "    \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "    state = torch.from_numpy(state).float().to(device)\n",
    "    actor_local.eval()\n",
    "    with torch.no_grad():\n",
    "        action = actor_local(state).cpu().data.numpy()\n",
    "    actor_local.train()\n",
    "    if add_noise:\n",
    "        action +=noise.sample()\n",
    "    return np.clip(action, -1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Define Training of the Agent\n",
    "\n",
    "Or go straight to step 8 to test agent loaded with previuous trained weights (in its Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train Agent - One of the core logic of DDPG - To TRAIN Actor and Critic Neural NETWORK\n",
    "def train_agent(state, action, reward, next_state, done, timestep, \n",
    "                actor_local, actor_target, actor_optimizer, critic_local, critic_target, critic_optimizer):\n",
    "    \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
    "    Q_targets = r + Î³ * critic_target(next_state, actor_target(next_state))\n",
    "    where:\n",
    "        actor_target(state) -> action\n",
    "        critic_target(state, action) -> Q-value\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "        experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "        gamma (float): discount factor\n",
    "    \"\"\"\n",
    "    \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
    "       \n",
    "    e = experience(state, action, reward, next_state, done)\n",
    "    # Save experience / reward\n",
    "    replay_memory.append(e)\n",
    "        \n",
    "    # Learn, if enough samples are available in memory\n",
    "    if len(replay_memory) > BATCH_SIZE and timestep % UPDATE_EVERY == 0:\n",
    "        for _ in range(UPDATE_TIMES):\n",
    "            \n",
    "            experiences = sample(replay_memory, BATCH_SIZE)\n",
    "            states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "            # ---------------------------- update critic ---------------------------- #\n",
    "            # Get predicted next-state actions and Q values from target models\n",
    "            actions_next = actor_target(next_states)\n",
    "            Q_targets_next = critic_target(next_states, actions_next)\n",
    "            # Compute Q targets for current states (y_i)\n",
    "            Q_targets = rewards + (GAMMA * Q_targets_next * (1 - dones))\n",
    "            # Compute critic loss\n",
    "            Q_expected = critic_local(states, actions)\n",
    "            critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "            # Minimize the loss\n",
    "            critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            U.clip_grad_norm_(critic_local.parameters(), 1)\n",
    "            critic_optimizer.step()\n",
    "\n",
    "\n",
    "            # ---------------------------- update actor ---------------------------- #\n",
    "            # Compute actor loss\n",
    "            actions_pred = actor_local(states)\n",
    "            actor_loss = -critic_local(states, actions_pred).mean()\n",
    "            # Minimize the loss\n",
    "            actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            actor_optimizer.step()\n",
    "\n",
    "            # ----------------------- update target networks ----------------------- #\n",
    "            update_targets(critic_local, critic_target, TAU)\n",
    "            update_targets(actor_local, actor_target, TAU) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Main Function - Define and Run Training\n",
    "\n",
    "Or go straight to step 8 to test agent loaded with previous trained weights (in its Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\t Mean Score for Agent 1: -0.000, Mean Score for Agent 2: -0.001, Mean Score of both Agents: -0.001\n",
      "Episode 200\t Mean Score for Agent 1: -0.005, Mean Score for Agent 2: -0.005, Mean Score of both Agents: -0.005\n",
      "Episode 300\t Mean Score for Agent 1: -0.004, Mean Score for Agent 2: -0.006, Mean Score of both Agents: -0.005\n",
      "Episode 400\t Mean Score for Agent 1: -0.006, Mean Score for Agent 2: -0.004, Mean Score of both Agents: -0.005\n",
      "Episode 500\t Mean Score for Agent 1: 0.001, Mean Score for Agent 2: 0.000, Mean Score of both Agents: 0.001\n",
      "Episode 600\t Mean Score for Agent 1: 0.005, Mean Score for Agent 2: 0.007, Mean Score of both Agents: 0.006\n",
      "Episode 700\t Mean Score for Agent 1: 0.004, Mean Score for Agent 2: -0.003, Mean Score of both Agents: 0.001\n",
      "Episode 800\t Mean Score for Agent 1: 0.004, Mean Score for Agent 2: -0.003, Mean Score of both Agents: 0.001\n",
      "Episode 900\t Mean Score for Agent 1: 0.009, Mean Score for Agent 2: -0.005, Mean Score of both Agents: 0.002\n",
      "Episode 1000\t Mean Score for Agent 1: 0.006, Mean Score for Agent 2: 0.002, Mean Score of both Agents: 0.004\n",
      "Episode 1100\t Mean Score for Agent 1: 0.025, Mean Score for Agent 2: 0.008, Mean Score of both Agents: 0.017\n",
      "Episode 1200\t Mean Score for Agent 1: 0.020, Mean Score for Agent 2: 0.035, Mean Score of both Agents: 0.028\n",
      "Episode 1300\t Mean Score for Agent 1: 0.040, Mean Score for Agent 2: 0.037, Mean Score of both Agents: 0.038\n",
      "Episode 1400\t Mean Score for Agent 1: 0.073, Mean Score for Agent 2: 0.089, Mean Score of both Agents: 0.081\n",
      "Episode 1500\t Mean Score for Agent 1: 0.126, Mean Score for Agent 2: 0.151, Mean Score of both Agents: 0.138\n",
      "Episode 1600\t Mean Score for Agent 1: 0.189, Mean Score for Agent 2: 0.186, Mean Score of both Agents: 0.187\n",
      "Episode 1700\t Mean Score for Agent 1: 0.484, Mean Score for Agent 2: 0.479, Mean Score of both Agents: 0.482\n",
      "Episode 1800\t Mean Score for Agent 1: 0.538, Mean Score for Agent 2: 0.545, Mean Score of both Agents: 0.542\n",
      "Episode 1900\t Mean Score for Agent 1: 0.549, Mean Score for Agent 2: 0.546, Mean Score of both Agents: 0.548\n",
      "Episode 2000\t Mean Score for Agent 1: 0.582, Mean Score for Agent 2: 0.579, Mean Score of both Agents: 0.580\n",
      "Episode 2100\t Mean Score for Agent 1: 0.695, Mean Score for Agent 2: 0.688, Mean Score of both Agents: 0.691\n",
      "Episode 2200\t Mean Score for Agent 1: 0.835, Mean Score for Agent 2: 0.837, Mean Score of both Agents: 0.836\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def ddpg(n_episodes=4000, target_score=0.5, print_every=100):\n",
    "    #Some utility score variables\n",
    "    scores1 = []\n",
    "    scores2 = []\n",
    "    mean_scores = []   \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        #reset the environment, noise and score\n",
    "        env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "        state = env_info.vector_observations              # get current state\n",
    "        noise.reset()                                     # reset the noise        \n",
    "        #reset the score for each agent\n",
    "        score1 = 0\n",
    "        score2 = 0\n",
    "        #reset some utility counter\n",
    "        t = 0\n",
    "        while True: \n",
    "            t += 1\n",
    "            state1 = np.concatenate([state[0], [1]]).reshape((1,state.shape[1]+1))\n",
    "            state2 = np.concatenate([state[1], [-1]]).reshape((1,state.shape[1]+1))\n",
    "            # select action with exploration \n",
    "            action1 = agent(actor_local1, state1, add_noise=True) \n",
    "            action2 = agent(actor_local2, state2, add_noise=True) \n",
    "            #Step the agents actions into the environment, and get reward, next state, etc\n",
    "            env_info = env.step([action1, action2])[brain_name]\n",
    "            next_state = env_info.vector_observations\n",
    "            reward = env_info.rewards\n",
    "            done = env_info.local_done\n",
    "            #Get the next_state for each agent from the main next_state variable\n",
    "            next_state1 = np.concatenate([next_state[0], [1]])\n",
    "            next_state2 = np.concatenate([next_state[1], [-1]])\n",
    "            #Train both the agents now - Actor and Critic Neural Networks \n",
    "            train_agent(state1, action1, np.mean(reward), next_state1, done[0], t,\n",
    "                        actor_local1, actor_target1, actor_optimizer1, critic_local1, critic_target1, critic_optimizer1)                   \n",
    "            train_agent(state2, action2, np.mean(reward), next_state2, done[1], t,\n",
    "                        actor_local2, actor_target2, actor_optimizer2, critic_local1, critic_target1, critic_optimizer2)                   \n",
    "            #Make next_state as current state and continue\n",
    "            state = next_state\n",
    "            # add reward to current score of each agent\n",
    "            score1 += reward[0]\n",
    "            score2 += reward[1]                  \n",
    "            # exit loop when episode ends\n",
    "            if np.all(done):            \n",
    "                break     \n",
    "        \n",
    "        scores1.append(score1)\n",
    "        scores2.append(score2)\n",
    "        mean_scores.append(np.mean([score1, score2]))\n",
    "        mean1 = np.mean(scores1[-100:])\n",
    "        mean2 = np.mean(scores2[-100:])\n",
    "        mean_score = np.mean(mean_scores[-100:])\n",
    "        if i_episode % print_every==0:\n",
    "            print('Episode {}\\t Mean Score for Agent 1: {:.3f}, Mean Score for Agent 2: {:.3f}, Mean Score of both Agents: {:.3f}'.format(\n",
    "            i_episode, mean1, mean2, mean_score))\n",
    "            \n",
    "        if i_episode % 100 == 0:\n",
    "            torch.save(actor_local1.state_dict(), 'checkpoint_actor_1.pth')\n",
    "            torch.save(critic_local1.state_dict(), 'checkpoint_critic_1.pth')\n",
    "            torch.save(actor_local2.state_dict(), 'checkpoint_actor_2.pth')\n",
    "            torch.save(critic_local2.state_dict(), 'checkpoint_critic_2.pth')\n",
    "        if mean_score>1:\n",
    "            break\n",
    "\n",
    "    return mean_scores\n",
    "\n",
    "scores = ddpg()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores, 'r-')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Test Trained Agent in the Environment\n",
    "\n",
    "Once this cell is executed, we can watch the agent's performance.  A window should pop up that allows us to observe the agent, as it moves through the environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 2.5450000381097198\n",
      "Total steps in this episode: 990\n",
      "Total score (averaged over agents) this episode: 2.045000030659139\n",
      "Total steps in this episode: 793\n",
      "Total score (averaged over agents) this episode: 1.3950000209733844\n",
      "Total steps in this episode: 548\n",
      "Total score (averaged over agents) this episode: 0.1450000023469329\n",
      "Total steps in this episode: 73\n",
      "Total score (averaged over agents) this episode: 2.4950000373646617\n",
      "Total steps in this episode: 966\n"
     ]
    }
   ],
   "source": [
    "#Load the previous trained weights\n",
    "#Need to turn on the GPU to even test it. Otherwise torch load results in error. \n",
    "actor_local1.load_state_dict(torch.load('checkpoint_actor_1.pth'))\n",
    "critic_local1.load_state_dict(torch.load('checkpoint_critic_1.pth'))\n",
    "actor_local2.load_state_dict(torch.load('checkpoint_actor_2.pth'))\n",
    "critic_local2.load_state_dict(torch.load('checkpoint_critic_2.pth'))\n",
    "\n",
    "for i in range(5):  \n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    steps = 0\n",
    "    while True:\n",
    "        state1 = np.concatenate([states[0], [1]]).reshape((1,states.shape[1]+1))\n",
    "        state2 = np.concatenate([states[1], [-1]]).reshape((1,states.shape[1]+1))\n",
    "        # select action with learned agent \n",
    "        action1 = agent(actor_local1, state1, add_noise=False) \n",
    "        action2 = agent(actor_local2, state2, add_noise=False) \n",
    "        #Step the agents actions into the environment, and get reward, next state, etc\n",
    "        env_info = env.step([action1, action2])[brain_name]\n",
    "        next_states = env_info.vector_observations\n",
    "        rewards = env_info.rewards\n",
    "        dones = env_info.local_done \n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        #Make next_state as current state and continue\n",
    "        states = next_states \n",
    "        steps += 1\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))\n",
    "    print('Total steps in this episode: {}'.format(steps))           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
